{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Task 2: Baseline for Multi-view rider intention prediction"],"metadata":{"id":"Y7kDAR14ADhu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cATPuAXGLRg9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#done for both test and train\n","import os\n","import numpy as np\n","\n","# Path to the base directory\n","base_dir = '/content/drive/My Drive/vgg_test_features'\n","\n","# Path to the output directory\n","output_base_dir = '/content/drive/My Drive/vgg_test_features_concatenated'\n","\n","# Main folders names\n","main_folders = ['Right', 'Left', 'Front']\n","\n","# Get the names of subfolders (assuming the same structure in all main folders)\n","subfolders = os.listdir(os.path.join(base_dir, main_folders[0]))\n","\n","# Function to load and concatenate .npy files from identical paths across different main folders\n","def concatenate_npy_files(base_dir, main_folders, subfolder, filename):\n","    arrays = []\n","    paths = []\n","    for folder in main_folders:\n","        file_path = os.path.join(base_dir, folder, subfolder, filename)\n","        arrays.append(np.load(file_path))\n","        paths.append(file_path)\n","    concatenated_array = np.concatenate(arrays, axis=0)\n","    return concatenated_array, paths, [array.shape for array in arrays]\n","\n","# Create output directory and subfolders if they don't exist\n","os.makedirs(output_base_dir, exist_ok=True)\n","for subfolder in subfolders:\n","    os.makedirs(os.path.join(output_base_dir, subfolder), exist_ok=True)\n","\n","# Iterate through subfolders and files to concatenate\n","for subfolder in subfolders:\n","    file_names = os.listdir(os.path.join(base_dir, main_folders[0], subfolder))\n","    for filename in file_names:\n","        concatenated_array, paths, shapes = concatenate_npy_files(base_dir, main_folders, subfolder, filename)\n","\n","        # Print the paths and initial dimensions\n","        print(f\"Concatenating the following files for {subfolder}/{filename}:\")\n","        for path, shape in zip(paths, shapes):\n","            print(f\" - {path}, shape: {shape}\")\n","\n","        # Print the final dimension\n","        print(f\"Final concatenated shape: {concatenated_array.shape}\")\n","\n","        # Save the concatenated array in the appropriate subfolder within the output directory\n","        save_path = os.path.join(output_base_dir, subfolder, filename)\n","        np.save(save_path, concatenated_array)\n","        print(f\"Saved concatenated array at {save_path}\\n\")\n","\n","print(\"All files concatenated and saved successfully.\")\n"],"metadata":{"id":"3keZD_3hSWEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Chanel-wise Fusion (optional - uncomment to use)**"],"metadata":{"id":"sh5Xd3x-ASYM"}},{"cell_type":"code","source":["# #channel-wise fusion\n","# import os\n","# import numpy as np\n","\n","# # Path to the base directory\n","# base_dir = '/content/drive/My Drive/vgg_train_features'\n","\n","# # Path to the output directory\n","# output_base_dir = '/content/drive/My Drive/vgg_train_features_cw'\n","\n","# # Main folders names\n","# main_folders = ['Right', 'Left', 'Front']\n","\n","# # Get the names of subfolders (assuming the same structure in all main folders)\n","# subfolders = os.listdir(os.path.join(base_dir, main_folders[0]))\n","\n","# # Function to load and stack .npy files from identical paths across different main folders\n","# def stack_npy_files(base_dir, main_folders, subfolder, filename):\n","#     arrays = []\n","#     paths = []\n","#     for folder in main_folders:\n","#         file_path = os.path.join(base_dir, folder, subfolder, filename)\n","#         arrays.append(np.load(file_path))\n","#         paths.append(file_path)\n","#     stacked_array = np.stack(arrays, axis=-1)  # Stack along a new axis (channel-wise)\n","#     return stacked_array, paths, [array.shape for array in arrays]\n","\n","# # Create output directory and subfolders if they don't exist\n","# os.makedirs(output_base_dir, exist_ok=True)\n","# for subfolder in subfolders:\n","#     os.makedirs(os.path.join(output_base_dir, subfolder), exist_ok=True)\n","\n","# # Iterate through subfolders and files to stack\n","# for subfolder in subfolders:\n","#     file_names = os.listdir(os.path.join(base_dir, main_folders[0], subfolder))\n","#     for filename in file_names:\n","#         stacked_array, paths, shapes = stack_npy_files(base_dir, main_folders, subfolder, filename)\n","\n","#         # Print the paths and initial dimensions\n","#         print(f\"Stacking the following files for {subfolder}/{filename}:\")\n","#         for path, shape in zip(paths, shapes):\n","#             print(f\" - {path}, shape: {shape}\")\n","\n","#         # Print the final dimension\n","#         print(f\"Final stacked shape: {stacked_array.shape}\")\n","\n","#         # Save the stacked array in the appropriate subfolder within the output directory\n","#         save_path = os.path.join(output_base_dir, subfolder, filename)\n","#         np.save(save_path, stacked_array)\n","#         print(f\"Saved stacked array at {save_path}\\n\")\n","\n","# print(\"All files stacked and saved successfully.\")\n"],"metadata":{"id":"_SmwmstzVDyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/tensorflow/docs"],"metadata":{"id":"gVseXJrubH4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow_docs.vis import embed\n","from tensorflow import keras\n","from imutils import paths\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import imageio\n","import cv2\n","import os"],"metadata":{"id":"q7CluuTJbKab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = os.listdir('/content/drive/MyDrive/vgg_train_features_concatenated')\n","\n","label_types = os.listdir('/content/drive/MyDrive/vgg_train_features_concatenated')\n","print (label_types)"],"metadata":{"id":"LUvEhURfcmgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rooms = []\n","\n","for item in dataset_path:\n"," # Get all the file names\n"," all_rooms = os.listdir('/content/drive/MyDrive/vgg_train_features_concatenated' + '/' +item)\n","\n"," # Add them to the list\n"," for room in all_rooms:\n","    rooms.append((item, str('/content/drive/MyDrive/vgg_train_features_concatenated' + '/' +item) + '/' + room))\n","\n","# Build a dataframe\n","train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n","print(train_df.head())\n","print(train_df.tail())"],"metadata":{"id":"AvhBiRYUcx2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = train_df.loc[:,['video_name','tag']]\n","df\n","df.to_csv('/content/drive/MyDrive/train_v.csv')"],"metadata":{"id":"tY6Yd0GCc--V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = os.listdir('/content/drive/MyDrive/vgg_test_features_concatenated')\n","print(dataset_path)\n","\n","room_types = os.listdir('/content/drive/MyDrive/vgg_test_features_concatenated')\n","print(\"Types of activities found: \", len(dataset_path))\n","\n","rooms = []\n","\n","for item in dataset_path:\n"," # Get all the file names\n"," all_rooms = os.listdir('/content/drive/MyDrive/vgg_test_features_concatenated' + '/' +item)\n","\n"," # Add them to the list\n"," for room in all_rooms:\n","    rooms.append((item, str('/content/drive/MyDrive/vgg_test_features_concatenated' + '/' +item) + '/' + room))\n","\n","# Build a dataframe\n","test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n","print(test_df.head())\n","print(test_df.tail())\n","\n","df = test_df.loc[:,['video_name','tag']]\n","df\n","df.to_csv('/content/drive/MyDrive/test_v.csv')"],"metadata":{"id":"BWuPj9XxdKhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(\"/content/drive/MyDrive/train_v.csv\")\n","test_df = pd.read_csv(\"/content/drive/MyDrive/test_v.csv\")\n","\n","print(f\"Total videos for training: {len(train_df)}\")\n","print(f\"Total videos for testing: {len(test_df)}\")\n","\n","\n","train_df.sample(10)"],"metadata":{"id":"7WY9HV4MddBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n","print(label_processor.get_vocabulary())\n","\n","labels = train_df[\"tag\"].values\n","labels = label_processor(labels[..., None]).numpy()\n","labels"],"metadata":{"id":"7A3B4cpCdl3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#hyperparameters\n","IMG_SIZE = 224\n","BATCH_SIZE = 64\n","EPOCHS = 100\n","MAX_SEQ_LENGTH = 20\n","NUM_FEATURES = 512"],"metadata":{"id":"a0WjmATQbTwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sequence_model():\n","    class_vocab = label_processor.get_vocabulary()\n","\n","    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n","    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n","\n","    # Refer to the following tutorial to understand the significance of using `mask`:\n","    # https://keras.io/api/layers/recurrent_layers/gru/\n","    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n","    x = keras.layers.GRU(8)(x)\n","    x = keras.layers.Dropout(0.4)(x)\n","    x = keras.layers.Dense(8, activation=\"relu\")(x)\n","    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n","\n","    rnn_model = keras.Model([frame_features_input, mask_input], output)\n","\n","    rnn_model.compile(\n","        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n","    )\n","    return rnn_model"],"metadata":{"id":"IuTJL3EAdy0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 30"],"metadata":{"id":"-2w5Tq43d4kp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the function to prepare all videos\n","def prepare_all_videos(df, root_dir):\n","    num_samples = len(df)\n","    video_paths = df[\"video_name\"].values.tolist()\n","    labels = df[\"tag\"].values\n","\n","    # Convert class labels to label encoding\n","    labels = label_processor(labels[..., None]).numpy()\n","\n","    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n","    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n","\n","    # For each video\n","    for idx, path in enumerate(video_paths):\n","        # Load concatenated features from the appropriate file\n","        features_path = os.path.join(root_dir, path)\n","        features = np.load(features_path, allow_pickle = True)\n","        features = np.squeeze(features)\n","\n","        # Extract the features and masks\n","        num_frames = min(MAX_SEQ_LENGTH, features.shape[0])\n","        frame_features[idx, :num_frames, :] = features[:num_frames, :]\n","        frame_masks[idx, :num_frames] = 1  # 1 = not masked, 0 = masked\n","\n","    return (frame_features, frame_masks), labels\n","\n","\n","# Define the function to run the experiment\n","def run_experiment(train_data, train_labels, test_data, test_labels):\n","    filepath = \"./tmp/video_classifier\"\n","    checkpoint = keras.callbacks.ModelCheckpoint(\n","        filepath, save_weights_only=True, save_best_only=True, verbose=1\n","    )\n","\n","    seq_model = get_sequence_model()\n","    history = seq_model.fit(\n","        [train_data[0], train_data[1]],\n","        train_labels,\n","        validation_split=0.3,\n","        epochs=EPOCHS,\n","        callbacks=[checkpoint],\n","    )\n","\n","    seq_model.load_weights(filepath)\n","    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","\n","    return history, seq_model\n","\n","\n","# Load and prepare training and testing data\n","train_data, train_labels = prepare_all_videos(train_df, \"/content/drive/MyDrive/vgg_train_features_concatenated\")\n","test_data, test_labels = prepare_all_videos(test_df, \"/content/drive/MyDrive/vgg_test_features_concatenated\")\n","\n","# Run the experiment\n","_, sequence_model = run_experiment(train_data, train_labels, test_data, test_labels)\n"],"metadata":{"id":"hJmnq9ulasue"},"execution_count":null,"outputs":[]}]}